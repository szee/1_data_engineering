# 1_data_engineering

## The thinking:

[randomuser.me](randomuser.me) is an API providing randomized user data.

The response is a nested json, so in order to make the data accessible we need to normalize it.

```
{
   "results":[
      {
         "gender":"male",
         "name":{
            "title":"Mr",
            "first":"Florian",
            "last":"Pierre"
         },
         "location":{
            "street":{
               "number":6120,
               "name":"Rue de Cuire"
            },
            "city":"Saint-Denis",
            "state":"Haute-Sa√¥ne",
            "country":"France",
            "postcode":70961,
            "coordinates":{
               "latitude":"39.7481",
               "longitude":"-140.8681"
            },
            "timezone":{
               "offset":"+3:00",
               "description":"Baghdad, Riyadh, Moscow, St. Petersburg"
            }
         },
         "email":"florian.pierre@example.com",
         "dob":{
            "date":"1972-01-02T21:51:51.139Z",
            "age":50
         },
         "id":{
            "name":"INSEE",
            "value":"1NNaN62534442 13"
         },
         "picture":{
            "large":"https://randomuser.me/api/portraits/men/75.jpg",
            "medium":"https://randomuser.me/api/portraits/med/men/75.jpg",
            "thumbnail":"https://randomuser.me/api/portraits/thumb/men/75.jpg"
         },
         "nat":"FR"
      }
   ],
   "info":{
      "seed":"flightright",
      "results":1,
      "page":1,
      "version":"1.3"
   }
}
```

After normalization we can already save the data in CSV fromat and calculate any statistics.

Also after normalization of the response we can see that that if we want to store the data to the relational data base we may just store it as it is usning the following DDL:

```
DROP TABLE IF EXISTS person;

CREATE TABLE person (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    gender TEXT,
    email TEXT,
    nat VARCHAR(2),
    name_title VARCHAR(4),
    name_first TEXT,
    name_last TEXT,
    location_street_number INTEGER,
    location_street_name TEXT,
    location_city TEXT,
    location_state TEXT,
    location_country TEXT,
    location_postcode TEXT,
    location_coordinates_latitude REAL,
    location_coordinates_longitude REAL,
    location_timezone_offset INTEGER,
    location_timezone_description TEXT,
    dob_date TIMESTAMP,
    dob_age INTEGER,
    id_name TEXT,
    id_value TEXT,
    picture_large TEXT,
    picture_medium TEXT,
    picture_thumbnail TEXT
);
```

However we can also devide this table into 3 as in the following DDL:
```
DROP TABLE IF EXISTS locations;

CREATE TABLE locations (
    location_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    location_street_number INTEGER,
    location_street_name TEXT,
    location_city TEXT,
    location_state TEXT,
    location_country TEXT,
    location_postcode TEXT,
    location_coordinates_latitude REAL,
    location_coordinates_longitude REAL,
    location_timezone_offset INTEGER,
    location_timezone_description TEXT,
);

DROP TABLE IF EXISTS pictures;

CREATE TABLE pictures (
    picture_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    picture_large TEXT,
    picture_medium TEXT,
    picture_thumbnail TEXT
);

DROP TABLE IF EXISTS persons;

CREATE TABLE persons (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    gender TEXT,
    email TEXT,
    nat VARCHAR(2),
    name_title VARCHAR(4),
    name_first TEXT,
    name_last TEXT,
    dob_date TIMESTAMP,
    dob_age INTEGER,
    id_name TEXT,
    id_value TEXT,
    location_id INTEGER references locations(location_id),
    picture_id INTEGER references pictures(picture_id)
);
```

## The app
This app is dockerized so you can just install docker, clone this repository, perform
```
docker-compose build
docker-compose up
```
and you will have a postgresql db set up with the first DDL suggested above and populated with the result of the response.

Also you can find a file out.csv in the root directory of the project with the results of the request and statistics.csv with the required statistics.
There is a dashboard visualizing statistics which could be found here: [http://localhost:8080/](http://localhost:8080/)
![Alt text](dasboard.jpg?raw=true "Optional Title")
There are two more ways to use this app without docker and with limited functionality:

You can clone the repo and perform 
```
pip install -r requirements.txt
python dash_pipeline.py
```
This way you will get a CSVs with data and statistics and a dashboard but the db will not be set up.

The third way will get you CSVs with data and statistics but not a dashboard.
```
pip install -r requirements.txt
python pipe_line_workers.py
```

## What to could be improved
### The app
- Add some functionality to the app to be able to get more data (not just 300 entries), to get users from other countries and use other API parameters such as gender.
- Add other visualization tools such as pivot table.
- Right now the app getting the data right from the API. It might be better to split it into 2 processes: data loading to db, visualizing the data stored in db.
### The db
- Add the ability to populate db with more data and add only new entries into the table (i.e. checking if the email is already stored in the db then skip this person).
- If needed use the 3 table schema.
- It is better and more secure to store the data on the separate db server and put db credentials in the separate file.
